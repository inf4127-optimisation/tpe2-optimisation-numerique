# ğŸ“‰ TPE : Algorithmes d'Optimisation NumÃ©rique Sans Contraintes

**UniversitÃ© de YaoundÃ© I** **DÃ©partement d'Informatique** **Cours :** INF4127 - Optimisation 2  
**Enseignant :** Pr. MELATAGIA  
**AnnÃ©e AcadÃ©mique :** 2024-2025

---

## ğŸ‘¥ Membres du Groupe
| Matricule | Nom & PrÃ©nom | RÃ´le / Fonction TraitÃ©e |
|-----------|--------------|-------------------------|
| **24F2456** | **ESSUTHI MBANGUE Ange Armel** | **Rosenbrock & Architecture GitHub** |
| **19M2351**| **TAGNE TALLA Idriss** | **Fonction de Himmelblau(Point Selle)** |
| **22W2163** | **DJATCHE-NKAMGANG Sylvano** | **Fonction Quadratique** |
| **21T2899** | **GOUJOU GUIMATSA Zidane** | **Analyse et Rapports** |

---

## ğŸ“ Description du Projet

Ce dÃ©pÃ´t contient les travaux pratiques encadrÃ©s (TPE) portant sur l'implÃ©mentation et l'analyse des **mÃ©thodes de descente de gradient** pour l'optimisation numÃ©rique sans contraintes.

L'objectif est de minimiser des fonctions $f: \mathbb{R}^n \to \mathbb{R}$ en comparant deux approches :
1.  **MÃ©thode Ã  Pas Fixe :** $x_{k+1} = x_k - \alpha \nabla f(x_k)$
2.  **MÃ©thode de la Plus Profonde Descente (Steepest Descent) :** Le pas $\alpha_k$ est optimisÃ© Ã  chaque itÃ©ration via une recherche linÃ©aire (Line Search).

Le projet inclut des **visualisations dynamiques** (animations vidÃ©o) pour illustrer la convergence des algorithmes.

---

## ğŸ§ª Fonctions Ã‰tudiÃ©es

Nous avons analysÃ© le comportement des algorithmes sur trois fonctions aux topologies caractÃ©ristiques :

### 1. La Fonction de Rosenbrock ("VallÃ©e de la Banane")
$$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$$
* **Minimum Global :** $(1, 1)$ oÃ¹ $f=0$.
* **DifficultÃ© :** Le minimum se trouve dans une vallÃ©e Ã©troite, courbe et plate.
* **Observation :** La mÃ©thode Ã  pas fixe est lente mais stable. La mÃ©thode Ã  pas optimal gÃ©nÃ¨re des zig-zags (directions orthogonales) trÃ¨s inefficaces dans la vallÃ©e.

### 2. La Fonction Quadratique (Point Selle)
$$f(x, y) = x^2 - y^2$$
* **Point Critique :** $(0, 0)$ ($\nabla f = 0$).
* **DifficultÃ© :** Ce n'est ni un minimum ni un maximum. La fonction est convexe en $x$ et concave en $y$.
* **Observation :** L'algorithme diverge vers $-\infty$ le long de l'axe $y$. Cela illustre le danger des points selles et l'importance de la convexitÃ©.

### 3. La Fonction de Himmelblau (ou autre fonction traitÃ©e)
*(Ã€ complÃ©ter selon ce que tes amis ont fait, gÃ©nÃ©ralement c'est celle-ci :)*
$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$
* **ParticularitÃ© :** PossÃ¨de 4 minimums globaux identiques.
* **Observation :** Le point de convergence dÃ©pend uniquement du point de dÃ©part choisi (Bassin d'attraction).

---

## ğŸ“‚ Structure du DÃ©pÃ´t

```bash
TP-OPtimisation-Descente_De_Gradient/
â”œâ”€â”€ notebooks/                  # Codes sources Jupyter
â”‚   â”œâ”€â”€ Rosenbrock.ipynb        # Analyse complÃ¨te Rosenbrock (avec VidÃ©os)
â”‚   â”œâ”€â”€ Quadratique.ipynb       # Analyse Point Selle
â”‚   â””â”€â”€ Himmelblau.ipynb        # Analyse Multi-modale
â”œâ”€â”€ doc/                        # Cahiers de suivi individuels
â”‚   â”œâ”€â”€ 24F2456AngeArmelESSUTHI.md
â”‚   â”œâ”€â”€ MatriculeNomPrenom.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ images/                     # Captures d'Ã©cran et graphiques gÃ©nÃ©rÃ©s
â”œâ”€â”€ README.md
MIT Licence     #Fichier
        